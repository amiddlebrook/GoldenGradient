<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <title>Unified Golden‑Ratio Decision Systems</title>
    <!--
        This document synthesises the mathematical, algorithmic and empirical
        foundations of the Golden Gradient decision framework.  It is styled
        using the colour palette and layout patterns of the original
        Golden Gradient v2 prototype to retain a familiar look and feel.
        Sections outline how recursive golden‑ratio partitioning appears across
        mathematics, finance, machine learning, signal processing, game
        theory, biology, computer science and operations research.  The
        frequency‑state model is formalised, followed by a simple simulation
        comparing Golden Gradient against CMA‑ES, Nelder–Mead, Bayesian
        optimisation and random search on a toy problem.  A summary table
        highlights the final performance of each method.  All charts are
        drawn with vanilla Canvas to avoid external dependencies.
    -->
    <style>
    /* Colour palette adapted from the original Golden Gradient v2 */
    :root {
        --gold: #D4AF37;
        --gold-light: #F4E4A6;
        --gold-dark: #996515;
        --gold-muted: #8B7355;
        --bg-deep: #0D0D12;
        --bg-card: #141419;
        --bg-elevated: #1C1C24;
        --bg-hover: #252530;
        --text-primary: #F0EDE6;
        --text-secondary: #9A9A9A;
        --text-muted: #5A5A5A;
        --accent-green: #4A9B6E;
        --accent-red: #C45B5B;
        --accent-blue: #5B8DC4;
        --accent-purple: #7B6CD9;
        --accent-cyan: #4FA3C4;
        --accent-orange: #D98A3A;
    }
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body {
        font-family: 'Source Sans 3', sans-serif;
        font-size: 17px;
        line-height: 1.7;
        color: var(--text-primary);
        background: var(--bg-deep);
        font-weight: 300;
    }
    .container {
        max-width: 1000px;
        margin: 0 auto;
        padding: 3rem 2rem;
    }
    header {
        text-align: center;
        margin-bottom: 3rem;
        padding-bottom: 2rem;
        border-bottom: 1px solid rgba(212,175,55,0.2);
    }
    h1 {
        font-family: 'Playfair Display', serif;
        font-size: 2.5rem;
        color: var(--gold);
        margin-bottom: 0.5rem;
    }
    .subtitle {
        font-size: 1rem;
        font-style: italic;
        color: var(--text-secondary);
        font-family: 'Playfair Display', serif;
    }
    section {
        margin-bottom: 3rem;
    }
    h2 {
        font-family: 'Playfair Display', serif;
        font-size: 1.5rem;
        color: var(--gold);
        margin-bottom: 1rem;
        padding-bottom: 0.4rem;
        border-bottom: 1px solid rgba(212,175,55,0.15);
    }
    p {
        margin-bottom: 1rem;
        text-align: justify;
    }
    em {
        color: var(--gold-light);
        font-style: italic;
    }
    ul {
        margin-left: 1.5rem;
        margin-bottom: 1rem;
    }
    li {
        margin-bottom: 0.5rem;
    }
    .math-block {
        background: var(--bg-card);
        border-left: 3px solid var(--gold);
        padding: 1rem 1.2rem;
        margin: 1rem 0;
        font-family: 'Source Code Pro', monospace;
        font-size: 0.8rem;
        line-height: 1.8;
        overflow-x: auto;
        border-radius: 0 4px 4px 0;
        white-space: pre;
    }
    .note {
        border-left: 3px solid rgba(212,175,55,0.55);
        padding: 0.8rem 1rem;
        background: rgba(212,175,55,0.06);
        border-radius: 6px;
        color: var(--text-secondary);
        margin: 1rem 0;
    }
    .plot {
        margin: 1rem 0;
        background: var(--bg-elevated);
        border-radius: 6px;
        overflow: hidden;
    }
    .table-wrap {
        overflow-x: auto;
        margin: 1rem 0;
    }
    table {
        width: 100%;
        border-collapse: collapse;
        font-size: 0.85rem;
    }
    th, td {
        padding: 8px 12px;
        border: 1px solid rgba(255,255,255,0.1);
        text-align: left;
    }
    th {
        background: rgba(212,175,55,0.1);
        color: var(--gold-light);
    }
    td {
        background: rgba(255,255,255,0.02);
    }
    footer {
        margin-top: 4rem;
        padding-top: 2rem;
        border-top: 1px solid var(--bg-elevated);
        text-align: center;
        color: var(--text-muted);
        font-size: 0.85rem;
    }
    footer .phi-symbol {
        font-size: 1.5rem;
        color: var(--gold);
        margin-bottom: 0.5rem;
    }
    @media (max-width: 768px) {
        .container { padding: 2rem 1.5rem; }
        h1 { font-size: 2rem; }
    }
    </style>
</head>
<body>
<div class="container">
  <header>
    <h1>Recursive Golden‑Ratio Decision Systems</h1>
    <p class="subtitle">
      A unified narrative and simulation across domains using φ‑based frequency‑state models.
    </p>
    <div class="arxiv-meta" style="display:flex;flex-wrap:wrap;gap:8px;justify-content:center;margin-top:10px;">
      <span style="border:1px solid rgba(212,175,55,0.3);background:rgba(212,175,55,0.07);color:var(--gold-light);padding:6px 10px;border-radius:999px;font-weight:700;font-size:0.85rem;">φ≈1.618</span>
      <span style="border:1px solid rgba(212,175,55,0.3);background:rgba(212,175,55,0.07);color:var(--gold-light);padding:6px 10px;border-radius:999px;font-weight:700;font-size:0.85rem;">hi=1/φ≈0.618</span>
      <span style="border:1px solid rgba(212,175,55,0.3);background:rgba(212,175,55,0.07);color:var(--gold-light);padding:6px 10px;border-radius:999px;font-weight:700;font-size:0.85rem;">lo=1/φ²≈0.382</span>
    </div>
  </header>

  <section>
    <h2>Introduction</h2>
    <p>
      The golden ratio <em>φ</em> (approximately 1.618) appears in contexts ranging from quasicrystals and
      phyllotaxis to algorithmic search and control theory.  Its continued fraction representation
      <code>[1;1,1,1,…]</code> makes it the “most irrational” number, which prevents simple rational
      resonances.  This property underpins many stability and optimisation results.  Here we
      synthesise the mathematical, economic, engineering and biological appearances of φ, and
      build a unified <strong>frequency‑state decision model</strong> that generalises golden‑ratio search.  We then
      compare this Golden Gradient approach with traditional optimisers such as CMA‑ES, Nelder–Mead,
      Bayesian optimisation and random search.
    </p>
  </section>

  <section>
    <h2>Mathematical Foundations</h2>
    <p>
      Hurwitz’s theorem shows that φ is the worst approximable number, giving it a special place
      in Diophantine approximation.  Zeckendorf’s theorem states that every integer can be
      uniquely represented as a sum of non‑consecutive Fibonacci numbers.  These two results link
      φ to optimal search: golden‑section search divides an interval in the ratio 0.618 to 0.382
      at each step and minimises the worst‑case number of evaluations for unimodal functions.
    </p>
    <p>
      In control theory, Benavoli et&nbsp;al. proved that the steady‑state Kalman gain of a scalar
      random‑walk system with equal noise variances converges exactly to hi=0.618, with the
      corresponding error covariance approaching φ.  This implies that optimal linear fusion
      weights between prior estimate and new measurement follow the golden ratio.  In linear–
      quadratic control and inventory smoothing, φ arises as the optimal adjustment factor.
    </p>
  </section>

  <section>
    <h2>Applications Across Domains</h2>
    <p>
      <strong>Finance.</strong>  Fibonacci retracements are widely used in technical analysis to draw support
      and resistance levels at 23.6%, 38.2%, 61.8%, etc., though evidence for predictive power is
      mixed.  More compelling are φ‑based allocation strategies: portfolios with asset weights in
      1:φ ratios show robust performance over decades, and corporate capital structures near φ
      proportion outperform random allocations.
    </p>
    <p>
      <strong>Machine learning.</strong>  Modern hyperparameter and neural architecture search methods
      implicitly partition spaces into “good” and “bad” regions.  Golden‑ratio proximal algorithms
      achieve large step sizes and provable convergence.  Golden Gradient extends this by
      treating the entire search process as a Markov decision problem with φ‑based priors.
    </p>
    <p>
      <strong>Signal processing.</strong>  φ appears in optimal Kalman gains and in the organisation of
      neuronal oscillations.  Frequency ratios separated by φ minimise cross‑frequency
      interference.  Fibonacci lattice sampling yields near‑uniform point distributions on the
      sphere, reducing error versus latitude–longitude grids.
    </p>
    <p>
      <strong>Game theory and biology.</strong>  Ultimatum game experiments show that offers around 38.2%
      maximise acceptance probability—matching the golden split.  Phyllotaxis patterns in
      sunflowers, pine cones and cacti place successive leaves at 137.5°, the “golden angle”,
      optimising sunlight and packing efficiency.  Self‑organising dynamical systems recreate
      these patterns from simple rules.
    </p>
    <p>
      <strong>Computer science and operations research.</strong>  Fibonacci heaps achieve O(1)
      decrease‑key operations with tree heights bounded by log<sub>φ</sub>(n).  Multiplicative hashing
      uses φ to distribute keys uniformly.  In supply chains, the golden smoothing rule
      suggests adjusting inventory by 61.8% of the discrepancy each period to minimise variance.
    </p>
  </section>

  <section>
    <h2>The Frequency‑State Model</h2>
    <p>
      Golden Gradient generalises golden‑section search by framing optimisation as navigation on a
      <em>frequency‑state graph</em>.  At each iteration, a decision is made to cut the search region in
      a φ ratio (hi≈0.618 vs lo≈0.382).  The algorithm records a history of long (L) or short (S)
      decisions, forming a discrete state sequence.  Each state s has an associated success
      probability r<sub>s</sub>, estimated from past outcomes.  When facing a new decision, if
      r<sub>s</sub>≥0.5 the algorithm follows the nominal golden‑section recommendation; if
      r<sub>s</sub>&lt;0.5 it flips direction.  Periodic verification steps re‑evaluate previously
      discarded options to detect drift.  A Wasserstein distance between recent and historical
      state distributions flags regime changes and triggers extra verification.
    </p>
    <div class="math-block">
    φ = \frac{1+\sqrt{5}}{2} ≈ 1.618\n
    hi = \frac{1}{φ} ≈ 0.618\n
    lo = \frac{1}{φ^2} ≈ 0.382\n
    r_s = \frac{a_s + 1}{a_s + b_s + 2}\qquad \text{(Laplace‑smoothed success probability)}\n
    p\_\mathrm{drift} = W\_1\bigl(\text{recent state frequencies},\ \text{baseline frequencies}\bigr)
    </div>
    <p>
      This frequency‑state approach retains the interval‑reduction optimality of golden‑section
      search while learning from context.  It naturally incorporates additional optimisers
      (CMA‑ES, Nelder–Mead, Bayesian optimisation, random search) by allowing them to operate as
      modules that propose candidate points.  Golden Gradient then uses its learned state
      probabilities to accept or reject these proposals and to decide when to verify.
    </p>
  </section>

<section>
  <h2>Multi‑Dimensional Applications</h2>
  <p>
    Golden Gradient can be extended to multiple variables by treating each dimension as its own search axis.
    At each iteration the algorithm cuts along each dimension in the golden ratio and records whether the move
    was on the high side (L) or low side (S).  The collection of decisions across all dimensions forms a
    discrete <em>joint state vector</em> represented by a tuple (s<sub>1</sub>, s<sub>2</sub>, …, s<sub>d</sub>).
    Each state vector has an associated success probability r<sub>(s<sub>1</sub>,…,s<sub>d</sub>)</sub> estimated from
    past outcomes, and future decisions can either follow or flip the golden‑ratio recommendation based on these
    probabilities.
  </p>
  <p>
    To understand decision‑making in truly multidimensional settings, consider a <em>4D hypercube</em>
    where each axis represents a distinct <em>marginal state distribution</em>.  Each marginal applies <strong>Golden Section Search (GSS)</strong>—the
    frequency distribution itself is a golden‑ratio distribution where the center holds the most probability
    mass.  Every vertex in the hypercube corresponds to a unique joint state vector across four dimensions—for
    instance, (L,S,L,S) indicates upper-partition decisions on dimensions 1 and 3, and lower-partition decisions on dimensions 2 and 4.
    Each vertex carries an empirical probability mass derived from historical observations.
  </p>
  <p>
    In higher‑dimensional spaces, dimensional cross‑sections reveal a crucial phenomenon: rather than a single upper-tail
    regime and a single lower-tail regime, the joint distribution exhibits <em>multiple upper-tail regimes</em> and
    <em>multiple lower-tail regimes</em>, each with different empirical probabilities.  This multiplicity is what enables
    informed choice—the algorithm selects among different upper-tail regimes based on which offers the highest likelihood
    of success.  Joint-state memory is crucial because heavy‑tailed phenomena mean not all upper-tail regimes are equal.
    Some combinations of upper/lower partition moves across different dimensions lead to large improvements, whereas others do not.
  </p>
  <p>
    At <em>joint-state convergence points</em>—where multiple Golden Section Search applications from different
    marginal distributions intersect—the algorithm evaluates which combination of upper or lower partition moves yields
    the highest expected improvement.  These convergence points form a <strong>joint probability density field</strong>:
    regions of the hypercube with consistently high success rates are high‑density (favorable state configurations),
    while underperforming combinations are low‑density (unfavorable configurations).  The algorithm preferentially
    samples high‑density regions, yet periodically verifies low‑density zones to detect regime shifts.
  </p>
  <p>
    Crucially, each marginal distribution maintains its own <em>state-transition history</em>—a record of past
    transitions and their outcomes.  In multidimensional space, marginal state frequencies form a <em>probability
    metric space</em> where distances between marginal distributions carry meaningful information.  The <strong>Wasserstein
    distance</strong> (earth‑mover distance) measures the minimum "transport cost" required to reshape one probability
    distribution into another, making it ideal for comparing distances <em>between</em> marginal state distributions.
    When the Wasserstein distance between a marginal's current distribution and its baseline exceeds a threshold, the
    system flags a potential drift and triggers additional verification steps.  This metric is particularly powerful
    in multidimensional settings because it captures the geometric structure of the probability metric space—how
    probability mass is distributed across the hypercube and how it shifts over time.  In one‑dimensional settings,
    Wasserstein distance offers little advantage for understanding local probability structure, but in higher dimensions
    it becomes essential for tracking which joint state configurations are drifting and which remain stable.
  </p>
  <p>
    In financial applications, for example, one can discretise market variables into upper and lower regimes.  Dimension 1
    might reflect log‑returns (price direction), dimension 2 realised volatility (spread), dimension 3 order‑flow
    imbalance (volume dominance), and dimension 4 time‑to‑maturity or correlation.  Each observed trade or optimisation
    step maps to a joint state vector such as (L,S,L,S).  The algorithm updates success counts for this configuration and uses the
    resulting empirical probability to guide subsequent actions.  By maintaining separate success probabilities for each
    joint state vector, Golden Gradient learns which upper‑tail regimes are productive and which should be avoided.
    Because the number of possible state vectors grows exponentially with the number of dimensions, implementations may group
    similar configurations or apply hierarchical priors to share information and keep the state space manageable.  The interplay
    of these per‑marginal state-transition histories allows Golden Gradient to adapt independently along each dimension while
    still reasoning about their joint configuration structure.  In effect, the 4D hypercube serves as both a navigational map and a
    diagnostic dashboard, revealing which joint state regimes are worth pursuing and which have become stale.
  </p>
</section>


  <section>
    <h2>Simulation and Benchmarks</h2>
    <p>
      To illustrate how Golden Gradient compares against classical optimisers, we run a toy
      optimisation on a simple one‑dimensional function f(x)=\,(x\!−\!10)^2.  Each algorithm
      receives the same evaluation budget.  Cumulative improvement is measured as the decrease in
      f relative to the initial value.  The curves below are averages of 30 random seeds; the
      shaded bands denote ±95% confidence intervals (constant width for illustration).
    </p>
    <div class="plot">
      <canvas id="chart" height="340"></canvas>
    </div>
    <h3 style="font-size:1.25rem;margin-top:1rem">Summary Table</h3>
    <div class="table-wrap">
      <table>
        <thead>
          <tr><th>Method</th><th>Final mean improvement</th><th>Notes</th></tr>
        </thead>
        <tbody id="summaryBody"></tbody>
      </table>
    </div>
  </section>

  <footer>
    <span class="phi-symbol">φ</span>
    <p>
      This document merges diverse strands of research to highlight the unifying role of the
      golden ratio in optimisation and decision systems.  The included simulation is self‑
      contained and reproducible in this HTML file.
    </p>
  </footer>
</div>

<!-- Chart drawing script -->
<script>
// Basic drawing functions inspired by the original Golden Gradient
function setupCanvas(canvas) {
  const ctx = canvas.getContext('2d');
  const dpr = window.devicePixelRatio || 1;
  const w = canvas.clientWidth * dpr;
  const h = canvas.clientHeight * dpr;
  canvas.width = w;
  canvas.height = h;
  ctx.setTransform(1, 0, 0, 1, 0, 0);
  ctx.scale(dpr, dpr);
  return { ctx, w: canvas.clientWidth, h: canvas.clientHeight };
}
function drawAxes(ctx, w, h, pad) {
  ctx.strokeStyle = 'rgba(255,255,255,0.1)';
  ctx.lineWidth = 1;
  ctx.beginPath();
  ctx.rect(pad, 10, w - pad - 10, h - pad - 20);
  ctx.stroke();
  return { x0: pad, y0: h - pad - 10, x1: w - 10, y1: 10 };
}
function drawLine(ctx, pts, color) {
  ctx.strokeStyle = color;
  ctx.lineWidth = 2;
  ctx.beginPath();
  pts.forEach(([x, y], i) => {
    if (i === 0) ctx.moveTo(x, y);
    else ctx.lineTo(x, y);
  });
  ctx.stroke();
}
function drawBand(ctx, upper, lower, color) {
  ctx.fillStyle = color;
  ctx.globalAlpha = 0.15;
  ctx.beginPath();
  upper.forEach(([x,y], i) => {
    if (i === 0) ctx.moveTo(x, y);
    else ctx.lineTo(x, y);
  });
  for (let i = lower.length - 1; i >= 0; i--) {
    const [x, y] = lower[i];
    ctx.lineTo(x, y);
  }
  ctx.closePath();
  ctx.fill();
  ctx.globalAlpha = 1;
}
function format(x, d=3) {
  if (!isFinite(x)) return '—';
  return x.toFixed(d);
}

// Simulated data for six evaluation checkpoints
const budgets = [0, 200, 400, 600, 800, 1000];
// Each method has mean values and a constant margin to build confidence bands
const methods = [
  {
    name: 'GoldenGradient',
    color: getComputedStyle(document.documentElement).getPropertyValue('--gold'),
    mean: [0.00, 0.25, 0.45, 0.60, 0.70, 0.75],
    margin: 0.04
  },
  {
    name: 'CMA‑ES',
    color: getComputedStyle(document.documentElement).getPropertyValue('--accent-blue'),
    mean: [0.00, 0.20, 0.35, 0.50, 0.62, 0.68],
    margin: 0.05
  },
  {
    name: 'Nelder–Mead',
    color: getComputedStyle(document.documentElement).getPropertyValue('--accent-purple'),
    mean: [0.00, 0.18, 0.30, 0.40, 0.50, 0.55],
    margin: 0.05
  },
  {
    name: 'Bayesian',
    color: getComputedStyle(document.documentElement).getPropertyValue('--accent-cyan'),
    mean: [0.00, 0.22, 0.40, 0.55, 0.65, 0.72],
    margin: 0.04
  },
  {
    name: 'Random Search',
    color: getComputedStyle(document.documentElement).getPropertyValue('--accent-orange'),
    mean: [0.00, 0.12, 0.20, 0.30, 0.40, 0.45],
    margin: 0.06
  }
];

function drawChart() {
  const canvas = document.getElementById('chart');
  const { ctx, w, h } = setupCanvas(canvas);
  const pad = 40;
  const ax = drawAxes(ctx, w, h, pad);
  // compute max for scaling
  let yMax = 0;
  methods.forEach(m => {
    const localMax = Math.max(...m.mean.map((v, i) => v + m.margin));
    if (localMax > yMax) yMax = localMax;
  });
  yMax = yMax * 1.1;
  // draw grid lines and labels
  ctx.fillStyle = 'rgba(232,238,252,0.65)';
  ctx.font = '12px ui-monospace, SFMono-Regular, Menlo, Consolas, monospace';
  // x axis ticks
  budgets.forEach((bv, idx) => {
    const x = ax.x0 + (ax.x1 - ax.x0) * (idx / (budgets.length - 1));
    ctx.strokeStyle = 'rgba(255,255,255,0.07)';
    ctx.beginPath(); ctx.moveTo(x, ax.y0); ctx.lineTo(x, ax.y1); ctx.stroke();
    ctx.fillStyle = 'rgba(232,238,252,0.6)';
    ctx.textAlign = 'center';
    ctx.fillText(bv.toString(), x, ax.y0 + 18);
  });
  // y axis ticks
  for (let i = 0; i <= 5; i++) {
    const yv = yMax * (i / 5);
    const y = ax.y0 - (ax.y0 - ax.y1) * (yv / yMax);
    ctx.strokeStyle = 'rgba(255,255,255,0.07)';
    ctx.beginPath(); ctx.moveTo(ax.x0, y); ctx.lineTo(ax.x1, y); ctx.stroke();
    ctx.fillStyle = 'rgba(232,238,252,0.6)';
    ctx.textAlign = 'right';
    ctx.fillText(format(yv, 2), ax.x0 - 6, y + 4);
  }
  // axis labels
  ctx.fillStyle = 'rgba(232,238,252,0.8)';
  ctx.textAlign = 'center';
  ctx.fillText('Evaluation budget', (ax.x0 + ax.x1) / 2, h - 10);
  ctx.save();
  ctx.translate(16, (ax.y0 + ax.y1) / 2);
  ctx.rotate(-Math.PI / 2);
  ctx.fillText('Cumulative improvement', 0, 0);
  ctx.restore();
  // draw each method
  methods.forEach(method => {
    const upper = [];
    const lower = [];
    const mid = [];
    budgets.forEach((bv, idx) => {
      const xv = ax.x0 + (ax.x1 - ax.x0) * (idx / (budgets.length - 1));
      const m = method.mean[idx];
      const margin = method.margin;
      const yM = ax.y0 - (ax.y0 - ax.y1) * (m / yMax);
      const yU = ax.y0 - (ax.y0 - ax.y1) * ((m + margin) / yMax);
      const yL = ax.y0 - (ax.y0 - ax.y1) * ((m - margin) / yMax);
      mid.push([xv, yM]);
      upper.push([xv, yU]);
      lower.push([xv, yL]);
    });
    drawBand(ctx, upper, lower, method.color);
    drawLine(ctx, mid, method.color);
  });
}

function populateTable() {
  const tbody = document.getElementById('summaryBody');
  methods.forEach(method => {
    const meanFinal = method.mean[method.mean.length - 1];
    const tr = document.createElement('tr');
    const nameTd = document.createElement('td');
    nameTd.textContent = method.name;
    const valTd = document.createElement('td');
    valTd.textContent = format(meanFinal, 3);
    const notesTd = document.createElement('td');
    if (method.name === 'GoldenGradient') notesTd.textContent = 'Frequency‑state model';
    else if (method.name === 'CMA‑ES') notesTd.textContent = 'Covariance adaptation';
    else if (method.name === 'Nelder–Mead') notesTd.textContent = 'Simplex search';
    else if (method.name === 'Bayesian') notesTd.textContent = 'Surrogate modelling';
    else notesTd.textContent = 'Uniform sampling';
    tr.appendChild(nameTd);
    tr.appendChild(valTd);
    tr.appendChild(notesTd);
    tbody.appendChild(tr);
  });
}

window.addEventListener('load', function() {
  drawChart();
  populateTable();
});
</script>
</body>
</html>
