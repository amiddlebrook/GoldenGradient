
\documentclass[twocolumn]{article}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{booktabs}

\title{Golden Gradient: Directional Gating as a Stability Heuristic for Gradient-Based Optimization}
\author{Avery Middlebrook}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We introduce \emph{Golden Gradient}, a heuristic optimizer that modifies gradient descent by applying directional gating prior to step application.
Unlike adaptive optimizers such as Adam and RMSProp, which rescale gradient magnitudes using moment estimates, Golden Gradient preserves raw gradient geometry and selectively accepts updates based on sustained directional alignment.
The method is explicitly positioned as a stability heuristic rather than a universally convergent optimizer.
On oscillatory and anisotropic loss surfaces, Golden Gradient accumulates monotonic gains while Adam and RMSProp exhibit limit cycles or divergence.
\end{abstract}

\section{Introduction}
Adaptive optimizers dominate modern machine learning due to fast initial convergence.
However, their reliance on moment-based normalization implicitly assumes locally stationary curvature.
When curvature rotates or gradients oscillate orthogonally, this assumption fails, producing zig-zagging or divergence.
Golden Gradient intervenes \emph{before} step application, rejecting directionally destructive updates without modifying magnitude.

\section{Heuristic Positioning}
Golden Gradient makes no claims of global convergence, regret bounds, or optimality.
It is a deterministic stability heuristic designed to suppress oscillations in hostile landscapes.
All claims are empirical and geometric rather than asymptotic.

\section{Method}
Let $g_t = \nabla f(x_t)$.
Define directional consistency:
\begin{equation}
C_t = \frac{\langle g_t, g_{t-1} \rangle}{\|g_t\|\|g_{t-1}\|}.
\end{equation}
An update is accepted iff
\begin{equation}
C_t \ge \tau,
\end{equation}
with $\tau = 1/\varphi$, $\varphi \approx 1.618$.
Rejected updates leave parameters unchanged.

\section{Algorithm}
\begin{algorithm}[H]
\caption{Golden Gradient}
\begin{algorithmic}[1]
\STATE Initialize $x_0$, learning rate $\eta$
\FOR{$t=1$ to $T$}
\STATE Compute gradient $g_t$
\STATE Compute $C_t$
\IF{$C_t \ge 1/\varphi$}
\STATE $x_{t+1} \leftarrow x_t - \eta g_t$
\ELSE
\STATE $x_{t+1} \leftarrow x_t$
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\section{Why the Golden Ratio}
Threshold ablations reveal three regimes:
(i) $\tau > 1/\varphi$: excessive rejection, slow progress;
(ii) $\tau < 1/\varphi$: oscillatory reversals admitted;
(iii) $\tau = 1/\varphi$: maximal net gain accumulation.
The golden ratio acts as a stability constant separating constructive descent from corrective backtracking.

\section{Why Adam Fails Here}
Adam rescales gradients by inverse second-moment estimates.
In oscillatory valleys this amplifies orthogonal noise, suppresses curvature-aligned motion, and induces persistent zig-zag trajectories.
Golden Gradient preserves gradient geometry and intervenes only at the level of directional validity, eliminating variance amplification.

\section{Empirical Results}
On rotating-curvature synthetic loss surfaces, Golden Gradient accumulates monotonic cumulative gain.
Adam and RMSProp exhibit limit cycles under identical learning rates.
Results are reproduced deterministically via an interactive simulator.

\section{Convergence Discussion}
Golden Gradient may temporarily stall when directional consistency is low.
This non-progress prevents divergence and enables recovery once curvature aligns.
We interpret stalling as controlled stability rather than failure.

\section{Conclusion}
Directional gating alone can outperform adaptive magnitude scaling in hostile optimization regimes.
Golden Gradient demonstrates that geometric stability constants can substitute for moment estimation.

\appendix
\section{Reproducibility}
Simulator and source code:
\url{https://amiddlebrook.github.io/GoldenGradient/}

\end{document}
